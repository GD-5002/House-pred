{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing the basic librarires\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from IPython.display import display\n",
        "\n",
        "#from brokenaxes import brokenaxes\n",
        "from statsmodels.formula import api\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10,6]\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7PLbA9FTsPRS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "jest563QsNeD",
        "outputId": "12b8288e-1c49-4f06-db2f-00732dd88bdd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Deepak\\\\Downloads\\\\Housing.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fe16c157fa57>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'C:\\Users\\Deepak\\Downloads\\Housing.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'price'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moriginal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Deepak\\\\Downloads\\\\Housing.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(r'C:\\Users\\Deepak\\Downloads\\Housing.csv',header=0)\n",
        "target = 'price'\n",
        "features = [i for i in df.columns if i not in [target]]\n",
        "original_df = df.copy(deep=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the dtypes of all the columns\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "kMrsR3TLsTMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking number of unique rows in each feature\n",
        "\n",
        "df.nunique().sort_values()"
      ],
      "metadata": {
        "id": "G5-x49kZsTPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking number of unique rows in each feature\n",
        "\n",
        "nu = df[features].nunique().sort_values()\n",
        "nf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features\n",
        "\n",
        "for i in range(df[features].shape[1]):\n",
        "    if nu.values[i]<=16:cf.append(nu.index[i])\n",
        "    else: nf.append(nu.index[i])\n",
        "\n",
        "print('\\n\\033[1mInference:\\033[0m The Datset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))"
      ],
      "metadata": {
        "id": "utn80oC_sTWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "YI2I_cNtsTYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removal of any Duplicate rows (if any)\n",
        "df.drop_duplicates(inplace=True)\n"
      ],
      "metadata": {
        "id": "u5Pnm1FrsTaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for empty elements\n",
        "\n",
        "nvc = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])\n",
        "nvc['Percentage'] = round(nvc['Total Null Values']/df.shape[0],3)*100\n",
        "print(nvc)"
      ],
      "metadata": {
        "id": "UQdb3z9usTc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting categorical Columns to Numeric\n",
        "\n",
        "df3 = df.copy()\n",
        "\n",
        "ecc = nvc[nvc['Percentage']!=0].index.values\n",
        "fcc = [i for i in cf if i not in ecc]\n",
        "#One-Hot Binay Encoding\n",
        "oh=True\n",
        "dm=True\n",
        "for i in fcc:\n",
        "    #print(i)\n",
        "    if df3[i].nunique()==2:\n",
        "        if oh==True: print(\"\\033[1mOne-Hot Encoding on features:\\033[0m\")\n",
        "        print(i);oh=False\n",
        "        df3[i]=pd.get_dummies(df3[i], drop_first=True, prefix=str(i))\n",
        "    if (df3[i].nunique()>2 and df3[i].nunique()<17):\n",
        "        if dm==True: print(\"\\n\\033[1mDummy Encoding on features:\\033[0m\")\n",
        "        print(i);dm=False\n",
        "        df3 = pd.concat([df3.drop([i], axis=1), pd.DataFrame(pd.get_dummies(df3[i], drop_first=True, prefix=str(i)))],axis=1)\n",
        "\n",
        "df3.shape"
      ],
      "metadata": {
        "id": "Vxgp9S0QsTey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removal of outlier:\n",
        "\n",
        "df1 = df3.copy()\n",
        "\n",
        "features1 = nf\n",
        "\n",
        "for i in features1:\n",
        "    Q1 = df1[i].quantile(0.25)\n",
        "    Q3 = df1[i].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    df1 = df1[df1[i] <= (Q3+(1.5*IQR))]\n",
        "    df1 = df1[df1[i] >= (Q1-(1.5*IQR))]\n",
        "    df1 = df1.reset_index(drop=True)\n",
        "display(df1.head())\n",
        "print('\\n\\033[1mInference:\\033[0m\\nBefore removal of outliers, The dataset had {} samples.'.format(df3.shape[0]))\n",
        "print('After removal of outliers, The dataset now has {} samples.'.format(df1.shape[0]))"
      ],
      "metadata": {
        "id": "x-iZ1KaSsllE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final Dataset size after performing Preprocessing\n",
        "\n",
        "df = df1.copy()\n",
        "df.columns=[i.replace('-','_') for i in df.columns]\n",
        "\n",
        "plt.title('Final Dataset')\n",
        "plt.pie([df.shape[0], original_df.shape[0]-df.shape[0]], radius = 1, labels=['Retained','Dropped'], counterclock=False,\n",
        "        autopct='%1.1f%%', pctdistance=0.9, explode=[0,0], shadow=True)\n",
        "plt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1sAIEuoslnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data intro training & testing sets\n",
        "\n",
        "m=[]\n",
        "for i in df.columns.values:\n",
        "    m.append(i.replace(' ','_'))\n",
        "\n",
        "df.columns = m\n",
        "X = df.drop([target],axis=1)\n",
        "Y = df[target]\n",
        "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=100)\n",
        "Train_X.reset_index(drop=True,inplace=True)\n",
        "\n",
        "print('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)"
      ],
      "metadata": {
        "id": "_82-Cqrcslpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling (Standardization)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "print('\\033[1mStandardardization on Training set'.center(120))\n",
        "Train_X_std = std.fit_transform(Train_X)\n",
        "Train_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\n",
        "display(Train_X_std.describe())\n",
        "\n",
        "print('\\n','\\033[1mStandardardization on Testing set'.center(120))\n",
        "Test_X_std = std.transform(Test_X)\n",
        "Test_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\n",
        "display(Test_X_std.describe())"
      ],
      "metadata": {
        "id": "nlvTx4gIslsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing a Linear Regression model with statsmodels\n",
        "\n",
        "Train_xy = pd.concat([Train_X_std,Train_Y.reset_index(drop=True)],axis=1)\n",
        "a = Train_xy.columns.values\n",
        "\n",
        "API = api.ols(formula='{} ~ {}'.format(target,' + '.join(i for i in Train_X.columns)), data=Train_xy).fit()\n",
        "#print(API.conf_int())\n",
        "#print(API.pvalues)\n",
        "API.summary()"
      ],
      "metadata": {
        "id": "aK_qi2XCswyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "Trr=[]; Tss=[]; n=3\n",
        "order=['ord-'+str(i) for i in range(2,n)]\n",
        "#Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
        "#Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
        "\n",
        "DROP=[];b=[]\n",
        "\n",
        "for i in range(len(Train_X_std.columns)):\n",
        "    vif = pd.DataFrame()\n",
        "    X = Train_X_std.drop(DROP,axis=1)\n",
        "    vif['Features'] = X.columns\n",
        "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    vif['VIF'] = round(vif['VIF'], 2)\n",
        "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
        "    vif.reset_index(drop=True, inplace=True)\n",
        "    if vif.loc[0][1]>1:\n",
        "        DROP.append(vif.loc[0][0])\n",
        "        LR = LinearRegression()\n",
        "        LR.fit(Train_X_std.drop(DROP,axis=1), Train_Y)\n",
        "\n",
        "        pred1 = LR.predict(Train_X_std.drop(DROP,axis=1))\n",
        "        pred2 = LR.predict(Test_X_std.drop(DROP,axis=1))\n",
        "\n",
        "        Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
        "        Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n",
        "\n",
        "        #Trd.loc[i,'ord-'+str(k)] = round(np.sqrt(mean_squared_error(Train_Y, pred1)),2)\n",
        "        #Tsd.loc[i,'ord-'+str(k)] = round(np.sqrt(mean_squared_error(Test_Y, pred2)),2)\n",
        "\n",
        "print('Dropped Features --> ',DROP)\n",
        "plt.plot(Trr, label='Train RMSE')\n",
        "plt.plot(Tss, label='Test RMSE')\n",
        "#plt.ylim([19.75,20.75])\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GoGH3ynyswz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "Trr=[]; Tss=[]; n=3\n",
        "order=['ord-'+str(i) for i in range(2,n)]\n",
        "Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
        "Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
        "\n",
        "m=df.shape[1]-2\n",
        "for i in range(m):\n",
        "    lm = LinearRegression()\n",
        "    rfe = RFE(lm,n_features_to_select=Train_X_std.shape[1]-i)             # running RFE\n",
        "    rfe = rfe.fit(Train_X_std, Train_Y)\n",
        "\n",
        "    LR = LinearRegression()\n",
        "    LR.fit(Train_X_std.loc[:,rfe.support_], Train_Y)\n",
        "\n",
        "    pred1 = LR.predict(Train_X_std.loc[:,rfe.support_])\n",
        "    pred2 = LR.predict(Test_X_std.loc[:,rfe.support_])\n",
        "\n",
        "    Trr.append(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
        "    Tss.append(np.sqrt(mean_squared_error(Test_Y, pred2)))\n",
        "\n",
        "plt.plot(Trr, label='Train RMSE')\n",
        "plt.plot(Tss, label='Test RMSE')\n",
        "#plt.ylim([19.75,20.75])\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4d9zFhJbsw2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA().fit(Train_X_std)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "x_values = range(1, pca.n_components_+1)\n",
        "ax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance')\n",
        "ax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red')\n",
        "plt.plot([0,pca.n_components_+1],[0.9,0.9],'g--')\n",
        "ax.set_title('Explained variance of components')\n",
        "ax.set_xlabel('Principal Component')\n",
        "ax.set_ylabel('Explained Variance')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HOHUOqnhsw5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "Trr=[]; Tss=[]; n=3\n",
        "order=['ord-'+str(i) for i in range(2,n)]\n",
        "Trd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
        "Tsd = pd.DataFrame(np.zeros((10,n-2)), columns=order)\n",
        "m=df.shape[1]-1\n",
        "\n",
        "for i in range(m):\n",
        "    pca = PCA(n_components=Train_X_std.shape[1]-i)\n",
        "    Train_X_std_pca = pca.fit_transform(Train_X_std)\n",
        "    Test_X_std_pca = pca.fit_transform(Test_X_std)\n",
        "\n",
        "    LR = LinearRegression()\n",
        "    LR.fit(Train_X_std_pca, Train_Y)\n",
        "\n",
        "    pred1 = LR.predict(Train_X_std_pca)\n",
        "    pred2 = LR.predict(Test_X_std_pca)\n",
        "\n",
        "    Trr.append(round(np.sqrt(mean_squared_error(Train_Y, pred1)),2))\n",
        "    Tss.append(round(np.sqrt(mean_squared_error(Test_Y, pred2)),2))\n",
        "\n",
        "# plt.figure(figsize=[20,4.5])\n",
        "# plt.subplot(1,3,1)\n",
        "# sns.heatmap(Trd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max())\n",
        "# plt.title('Train RMSE')\n",
        "# plt.subplot(1,3,2)\n",
        "# sns.heatmap(Tsd.loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+10)\n",
        "# plt.title('Test RMSE')\n",
        "# plt.subplot(1,3,3)\n",
        "# sns.heatmap((Trd+Tsd).loc[:6], cmap='BuGn', annot=True, vmin=0, vmax=Trd.max().max()+25)\n",
        "# plt.title('Total RMSE')\n",
        "# plt.show()\n",
        "\n",
        "plt.plot(Trr, label='Train RMSE')\n",
        "plt.plot(Tss, label='Test RMSE')\n",
        "#plt.ylim([19.5,20.75])\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P0jvgC3rs_L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Shortlisting the selected Features (with RFE)\n",
        "\n",
        "lm = LinearRegression()\n",
        "rfe = RFE(lm,n_features_to_select=Train_X_std.shape[1]-5)             # running RFE\n",
        "rfe = rfe.fit(Train_X_std, Train_Y)\n",
        "\n",
        "LR = LinearRegression()\n",
        "LR.fit(Train_X_std.loc[:,rfe.support_], Train_Y)\n",
        "\n",
        "#print(Train_X_std.loc[:,rfe.support_].columns)\n",
        "\n",
        "pred1 = LR.predict(Train_X_std.loc[:,rfe.support_])\n",
        "pred2 = LR.predict(Test_X_std.loc[:,rfe.support_])\n",
        "\n",
        "print(np.sqrt(mean_squared_error(Train_Y, pred1)))\n",
        "print(np.sqrt(mean_squared_error(Test_Y, pred2)))"
      ],
      "metadata": {
        "id": "TEfiOC6hs_N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us first define a function to evaluate our models\n",
        "\n",
        "Model_Evaluation_Comparison_Matrix = pd.DataFrame(np.zeros([5,8]), columns=['Train-R2','Test-R2','Train-RSS','Test-RSS',\n",
        "                                                                            'Train-MSE','Test-MSE','Train-RMSE','Test-RMSE'])\n",
        "rc=np.random.choice(Train_X_std.loc[:,Train_X_std.nunique()>=50].columns.values,1,replace=False)\n",
        "def Evaluate(n, pred1,pred2):\n",
        "    #Plotting predicted predicteds alongside the actual datapoints\n",
        "    plt.figure(figsize=[15,6])\n",
        "    for e,i in enumerate(rc):\n",
        "        plt.subplot(2,3,e+1)\n",
        "        plt.scatter(y=Train_Y, x=Train_X_std[i], label='Actual')\n",
        "        plt.scatter(y=pred1, x=Train_X_std[i], label='Prediction')\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    #Evaluating the Multiple Linear Regression Model\n",
        "\n",
        "    print('\\n\\n{}Training Set Metrics{}'.format('-'*20, '-'*20))\n",
        "    print('\\nR2-Score on Training set --->',round(r2_score(Train_Y, pred1),20))\n",
        "    print('Residual Sum of Squares (RSS) on Training set  --->',round(np.sum(np.square(Train_Y-pred1)),20))\n",
        "    print('Mean Squared Error (MSE) on Training set       --->',round(mean_squared_error(Train_Y, pred1),20))\n",
        "    print('Root Mean Squared Error (RMSE) on Training set --->',round(np.sqrt(mean_squared_error(Train_Y, pred1)),20))\n",
        "\n",
        "    print('\\n{}Testing Set Metrics{}'.format('-'*20, '-'*20))\n",
        "    print('\\nR2-Score on Testing set --->',round(r2_score(Test_Y, pred2),20))\n",
        "    print('Residual Sum of Squares (RSS) on Training set  --->',round(np.sum(np.square(Test_Y-pred2)),20))\n",
        "    print('Mean Squared Error (MSE) on Training set       --->',round(mean_squared_error(Test_Y, pred2),20))\n",
        "    print('Root Mean Squared Error (RMSE) on Training set --->',round(np.sqrt(mean_squared_error(Test_Y, pred2)),20))\n",
        "    print('\\n{}Residual Plots{}'.format('-'*20, '-'*20))\n",
        "\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-R2']  = round(r2_score(Train_Y, pred1),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-R2']   = round(r2_score(Test_Y, pred2),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-RSS'] = round(np.sum(np.square(Train_Y-pred1)),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-RSS']  = round(np.sum(np.square(Test_Y-pred2)),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-MSE'] = round(mean_squared_error(Train_Y, pred1),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-MSE']  = round(mean_squared_error(Test_Y, pred2),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Train-RMSE']= round(np.sqrt(mean_squared_error(Train_Y, pred1)),20)\n",
        "    Model_Evaluation_Comparison_Matrix.loc[n,'Test-RMSE'] = round(np.sqrt(mean_squared_error(Test_Y, pred2)),20)\n",
        "\n",
        "    # Plotting y_test and y_pred to understand the spread.\n",
        "    plt.figure(figsize=[15,4])\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    sns.distplot((Train_Y - pred1))\n",
        "    plt.title('Error Terms')\n",
        "    plt.xlabel('Errors')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.scatter(Train_Y,pred1)\n",
        "    plt.plot([Train_Y.min(),Train_Y.max()],[Train_Y.min(),Train_Y.max()], 'r--')\n",
        "    plt.title('Test vs Prediction')\n",
        "    plt.xlabel('y_test')\n",
        "    plt.ylabel('y_pred')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "E0dU2bT4tEln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression\n",
        "\n",
        "MLR = LinearRegression().fit(Train_X_std,Train_Y)\n",
        "pred1 = MLR.predict(Train_X_std)\n",
        "pred2 = MLR.predict(Test_X_std)\n",
        "\n",
        "print('{}{}\\033[1m Evaluating Multiple Linear Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\n",
        "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
        "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
        "\n",
        "Evaluate(0, pred1, pred2)"
      ],
      "metadata": {
        "id": "WnMLDZPXtEpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the 5th Order Polynomial Regression model (degree=5)\n",
        "\n",
        "poly_reg = PolynomialFeatures(degree=5)\n",
        "X_poly = poly_reg.fit_transform(Train_X_std)\n",
        "X_poly1 = poly_reg.fit_transform(Test_X_std)\n",
        "PR = LinearRegression()\n",
        "PR.fit(X_poly, Train_Y)\n",
        "\n",
        "pred1 = PR.predict(X_poly)\n",
        "pred2 = PR.predict(X_poly1)\n",
        "\n",
        "print('{}{}\\033[1m Evaluating Polynomial Regression Model \\033[0m{}{}\\n'.format('<'*3,'-'*35 ,'-'*35,'>'*3))\n",
        "print('The Coeffecient of the Regresion Model was found to be ',MLR.coef_)\n",
        "print('The Intercept of the Regresion Model was found to be ',MLR.intercept_)\n",
        "\n",
        "Evaluate(4, pred1, pred2)"
      ],
      "metadata": {
        "id": "-Nj-EDdKtJ1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Regression Models Results Evaluation\n",
        "\n",
        "EMC = Model_Evaluation_Comparison_Matrix.copy()\n",
        "EMC.index = ['Multiple Linear Regression (MLR)','Ridge Linear Regression (RLR)','Lasso Linear Regression (LLR)','Elastic-Net Regression (ENR)','Polynomial Regression (PNR)']\n",
        "EMC"
      ],
      "metadata": {
        "id": "w7oe5rjNtJ40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# R2-Scores Comparison for different Regression Models\n",
        "\n",
        "R2 = round(EMC['Train-R2'].sort_values(ascending=True),4)\n",
        "plt.hlines(y=R2.index, xmin=0, xmax=R2.values)\n",
        "plt.plot(R2.values, R2.index,'o')\n",
        "plt.title('R2-Scores Comparison for various Regression Models')\n",
        "plt.xlabel('R2-Score')\n",
        "#plt.ylabel('Regression Models')\n",
        "for i, v in enumerate(R2):\n",
        "    plt.text(v+0.02, i-0.05, str(v*100), color='blue')\n",
        "plt.xlim([0,1.1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSKWiYhgs_RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_3GgS67tSId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RKKX7watSL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PbGIKkRWsTiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}